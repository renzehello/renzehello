{"meta":{"title":"RZ","subtitle":"","description":"","author":"黑泽暗","url":"http://renzehello.github.io","root":"/"},"posts":[{"tags":[{"name":"Bitmap","slug":"Bitmap","permalink":"http://renzehello.github.io/tags/Bitmap/"},{"name":"Architecture","slug":"Architecture","permalink":"http://renzehello.github.io/tags/Architecture/"}],"title":"BitBase在用户数据分析中的应用与实践","date":"2020/01/19","text":"本文内容来自 数据平台部数据架构组@陈杨 在 BigData NoSQL 12th MeetUp 中的演讲内容。 在快手 HBase 建设的近两年时间中，积累了比较丰富的应用场景：如短视频的存储、IM、直播里评论 feed 流等场景。本次介绍其中的一个场景：Bitbase在用户数据分析中的应用与实践。内容包括业务需求、解决方案、业务效果、未来规划和内部使用经验。 概要 业务需求及挑战：BitBase 引擎的初衷是什么； BitBase 解决方案：在 HBase 基础上，BitBase 的架构； 业务效果：在快手的实际应用场景中，效果如何； 未来规划：中短期的规划； 内部使用经验：快手内部的实际使用经验。 业务需求与挑战1. 业务需求 对于一种技术的应用，首先需要明确其解决的业务需求。本文阐述的技术相应的需求场景用一句话总结：在千亿级日志中，选择任意维度，秒级计算出设备7-90日留存。 如上图所示。左边是原始数据，可能跨90天，每一天的数据可以看作是一张 Hive 宽表，在逻辑上可以认为每行数据的 rowkey 是 userId，需要通过90天的原始数据计算得到右边的表，它的横轴和纵轴都是日期，每个格子表示纵轴日期相对于横轴日期的留存率。 该需求的挑战在于： 日志量大，千亿级； 任意维度，如 city、sex、喜好等，需要选择任意多个维度，在这些维度下计算留存率； 秒级计算，产品面向分析师，等待时间不能过长，最好在1-2秒。 2. 技术选型 调研的技术解决方案如下： Hive： 大部分数仓是基于hive建设的，选择hive的优势在于不需要做大规模数据的迁移和转换，缺点是计算时延过长，会达到小时级别。 ES：通过对原始数据进行倒排索引，然后做一个类似于计算UV的解决方案，但是在数据需要做精确去重的场景下，时延需要秒到分钟级别。 ClickHouse：ClickHouse 是一个比较合适的引擎，也是一个非常优秀的引擎，在业界被广泛应用于 APP 分析，比如漏斗，留存。但是在我们的测试的中，当机器数量比较少时 ( bitmap的格式数据。 2. BitBase架构 BitBase架构主要由五部分组成： 数据存储：存储数据主要是两种，一是bitmap索引及数据，二是转换字典的归档文件，主要用于deviceId和bitmap offsetId的转换。 数据转换：转换有两种方式：一是通过mr任务加工，一是在线计算或导入数据。 数据计算：负责计算任务的解析和调度，以及计算结果返回给client client：将计算逻辑封装成的业务接口，供调用方使用 ZK：负责整个架构的分布式管理 3. 存储模块 BitBase数据历经原始日志数据->Bitmap数据->Hbase表三个阶段的转换操作，最终存储在Hbase表中。Bitmap数据包括两部分： MetaData信息：唯一定位一个 bitmap，db 可以认为是 hive 中的 db，table 也可以认为是 hive 中的 table，event 表示维度 (如:城市)，eventv 表示维度值 (如:bj)，entity 表示 userId（也可能是 photoId），version 表示版本。 BitmapDataHbase表分为三种： BitmapMeta：保存 bitmap 的 meta 信息和一些 block 索引信息。 BlockData：直接保存BitmapData信息 BlockMeta：保存 block 的 meta 信息，起辅助作用。这里注意，相同db的bitmap数据存储在同一张Hbase表中，从而在bitmap计算的时候，可以通过db判断数据的亲和性，同一db的数据计算效率会比较高。 4. 计算模块 一个完整的计算流程包括client、BitBase Server和HBase RegionServer三部分。 BitBase Client将业务需求封装成计算表达式，然后将表达式发送给BitBase Server。从客户端使用的角度来看，bitbase的预留符号有&、|、^、!、@、$、#、%、*，用预留符号表示与、或、非、Count、List、Create等操作。 BitBase Server接收到表达式之后，首先访问BitmapMeta表，查询Block索引，然后将表达式切分成n个子表达式。 如果所有 bitmap 的 db 相同，则走 coprocessor 路由，否则按照数据亲和性，将 block 计算分发到其它 bitbaseServer 中。 根据第3步的调度策略，分两条不同的路径计算 block 表达式。 BitBase Server 聚合 block 计算表达式的结果，然后返回给 BitBase Client。两种计算方式的对比： 非本地计算，解决跨 db 计算的需求，它主要的瓶颈在于网卡和 GC。 本地计算，解决同 db 计算的需求，它主要的瓶颈在 CPU 和 GC 上。整体上看本地计算的性能比非本地计算的性能提高3-5倍，所以要尽量采用本地计算方式。 5. DeviceId问题 一句话描述问题：怎么高效实现字符串类型的DeviceId与long类型的offsetId之间的转换与反转换，并保证offsetId的一致性、连续性。 连续：deviceIdIndex 如果存在空洞，会降低压缩效率，同时 Block 数量会增加，计算复杂度相应增加，最终计算变慢； 一致：deviceId 和 deviceIdIndex 必须是一一对应的，否则计算结果不准确； 反解：根据 deviceIdIndex 能够准确、快速地反解成原始的 deviceId； 转换快：在亿级数据规模下，deviceId 转化为 deviceIdIndex 的过程不能太长。 6. DeviceId方案连续、一致、支持反解： 如何保证连续、一致、支持反解？解决方案非常简单，利用 HBase 实现两阶段提交协议。如上图中间实线部分所示，定义 deviceId 到 deviceIdIndex 的映射为字典。第一张表存储字典的 meta 信息；第二张表存储 index 到 deviceId 的映射；第三张表存储 deviceId 到 index 的映射。 生成 Index 的过程。举例说明, 假设我们已经生成了 1w 个 deviceId 映射，那么此时 f:max=1w，现在将新生成 1k 条映射： 将 f:nextMax=f:max+1k=1.1w； 写 Index 到 deviceId 的反向映射表，1k 条； 写 deviceId 到 Index 的正向映射表，1k 条； 把 f:max=f:nextMax=1.1w 更新到 meta 表，生成过程结束。 如果在生成过程中出现异常或服务器宕机，则执行回滚流程： 如果我们检测到 f:nextMax 不等于 f:max(f:nextMax>f:max)，则从表2中查询 max 到 nextMax 的数据，从表3中删掉相应的 deviceId 到 index 的映射记录； 再删掉表2中相应的 index 到 deviceId 的记录； 最后把 f:nextMax=f:max，从而实现数据100%一致。用 HBase 实现两阶段提交协议要求 index 生成流程和回滚流程一定是单线程的，从而出现性能瓶颈，所以 BitBase 设计了归档流程，以支持快速转换(见后面的描述)。Meta 表中有两个字段，如果发现新产生的数据大于 f:archive_num 就发起归档，把表3中的新数据直接写到 HDFS 中 archive_path 目录下。 采用上述方案与TD的spark程序给offset发号的方案相比，因为增加了监控和回滚方案，所以可靠性更高。 快速转化： 用mr join操作实现转换，转换分deviceId->offsetId和offsetId->deviceId两种，首先看deviceId->offsetId： 同时输入原始数据和字典归档数据，在 MRjob 中根据 deviceId 做 join；判断 deviceId 是否 join 成功； 如果成功了，直接写 hdfs，这样就得到了转化后的数据； 如果 join 失败，直接请求单实例 BitBase Master，BitBase Master 通过两阶段提交协议生成新的映射； 然后返回给 join task 执行替换 deviceId； 把转换后的数据写入 hdfs。 offsetId->deviceId反解的过程很简单，直接多并发读取 HBase。 业务效果1. 时间延迟 如上图所示，第一个图是，两维度、不同时间跨度计算留存的时间延迟；第二个图是15日留存在不同维度上的时延，时延并不会随着维度的增长而增长，原因是维度越多，表达式中可能不需要计算的 block 块也越多。 2. 服务现状 如上图所示，BitBase 可以应用在 app 分析，用户增长，广告 DMP，用户画像等多个业务场景中。 未来规划 根据现在面临的业务场景，BitBase 后续会在多个方面做优化。 支持实时聚合，在一些业务场景下，如运营效果监测，导入时效需要 构建bitmap”的重复操作，加重gc问题。下一步打算直接缓存bitmap。 数据过期与删除问题。在使用实践中，我们发现用户数据的过期方式有两种：第一种是按照bitmap的版本个数进行删除，比如: 只保留bitmap的最先1个版本；第二种是按照导入数据的时间，用TTL来判断过期。由于一个bitmap不能完全映射为一个HBase cell，所以这两种过期方式不能完全映射为HBase的多version控制模式和TTL过期模式。我们现在支持手动执行bitmap的version过期和TTL过期清理，维护成本比较高，后续会支持用户自助配置，系统自动清理的方式。 个人疑问 GreenPlum也能提供大数据量Bitmap的交叉运算及查询交互，不知道有没有对GP这种解决方案做过调研，或者是处于什么原因没有直接使用GP求解。个人思考：公司内部对GP并不是很了解，Hbase整体应用比较成熟，而且基于Hbase可以更加灵活的定制字典转换表等方案，更有利于后期功能和效率的改进。","permalink":"http://renzehello.github.io/2020/01/19/bitbase-in-kwai/","photos":[]},{"tags":[{"name":"Bitmap","slug":"Bitmap","permalink":"http://renzehello.github.io/tags/Bitmap/"}],"title":"Bitmap详解","date":"2020/01/18","text":"导读本文将以 Bitmap 在 TD 内部的使用场景为切入点，较为深入地探讨不同类型 BitMap 算法的存储结构和基本操作，同时会对不同 Bitmap 的优缺点进行简单介绍。希望通过对 Bitmap 的学习，帮助大家在使用bitmap 进行海量数据处理的时候，能够知其然更知其所以然。 参考文献[1] D.Lemire, O. Kaser, K. Aouiche, Sorting improves word-aligned bitmap indexes, Sorting improves word-aligned bitmap indexes.[2] S. Chambi, D. Lemire, O. Kaser, R. Godin, Better bitmap performance with Roaring bitmaps , Natural Sciences and Engineering Research Council of Canada.[3] D. Lemire, G. Ssi-Yan-Kai, O. Kaser, Consistently faster and smaller compressed bitmaps with Roaring, Natural Sciences and Engineering Research Council of Canada.[4] Jianguo Wang, Chunbin Lin, Yannis Papakonstantinou, Steven Swanson, An Experimental Study of Bitmap Compression vs. Inverted List Compression, SIGMOD ’17.[5] D. Lemire, O. Kaser, K. Aouiche, Sorting improves word-aligned bitmap indexes, Data & Knowledge Engineering.","permalink":"http://renzehello.github.io/2020/01/18/bitmap/","photos":[]},{"tags":[],"title":"image","date":"2020/01/18","text":"插入图片test1 test2 test3 test4官方办法 test5 test6 test7","permalink":"http://renzehello.github.io/2020/01/18/image/","photos":[]},{"tags":[{"name":"flink","slug":"flink","permalink":"http://renzehello.github.io/tags/flink/"}],"title":"Flink编程模型","date":"2020/01/17","text":"抽象级别Flink提供了不同的抽象级别来开发流/批处理程序。 最低级别抽象只提供有状态流（stateful streaming）。它将Process Function嵌入到DataStream API中。它允许用户自由处理来自一个或多个流的事件，并使用一致的容错状态。此外，用户可以注册事件时间和处理时间回调，允许程序实现复杂的计算。 实际上，一般应用并不需要stateful streaming这么底层的抽象，而是采用Core Apis即可，例如DataStrean API（有界/无界的流）和DataSet API（有界的数据集）。这些常用的APIs提供了常见的数据处理构建块（building blocks for data processing），比如用户指定的各种形式的转换、连接、聚合、窗口、状态等。在这些api中处理的数据类型用各自的编程语言表示为类。 Table API 是一个以数据表为中心的声明性DSL，表可以(在表示流时)动态地更改表。Table API遵循(扩展的)关系模型:表有一个附加的schema(类似于关系数据库中的表)，而API提供了类似的操作，如select、project、join、group-by、aggregate等。虽然可以通过UDF进行拓展，但是Table Apis的表现力仍然要比Core Apis要弱，不过它使用起来更简洁。此外Table Api在执行之前会经过规则优化器的优化。Table Apis可以在tables和DataStream/DataSet之间自由转换，允许Table API和DataStream/DataSet API混合使用。这些都很像spark的接口类型。 Flink提供的最高级抽象是SQL。这种抽象在语义和表达性上都类似于Table API，但将程序表示为SQL查询表达式。SQL抽象与Table API紧密交互，SQL查询可以在Table API中定义的表上执行。 这里的接口抽象和spark很像，过后可以在没种抽象下加一段示例代码 程序和数据流 结果上图来看，Flink程序可以视为一个有向无环图，图的起点是data source，节点是各种transfermation operator，边就是streaming，终点是data sink。流（streams）和转换（transformations）是构成Flink程序的基本构建块（The basic building blocks，基本组成）。需要注意的是，Flink的DataSet API也是基于流实现的——稍后会详细介绍。从概念上讲，流是一种（可能永远不会结束）流动的数据记录，而转换是一种操作，它接收一个或者多个流作为输入，并且生产出一个或多个输出流作为结果。当执行时，Flink程序被影射到流式数据流（streaming dataflows），由流和转换操作组成。每个数据流开始于一个或多个数据源（sources），并结束于一个或多个接收器（sinks）。数据流类似于任意有向无环图（directed acyclic graphs DAGs）。 并行数据流 从并行和分布式的视角看Flink程序的话，流–>流分区；转换操作–>操作子任务抽象视角 | 并行视角 | 分类|-|-|-stream | stream partitions | - |transfermation operator | operator subtasks | one to one / redistribution | Flink程序是并行和分布式的。在执行期间，一个流有一个或多个流分区（stream partitions），每个操作符有一个或多个操作子任务（operator subtasks）。operator subtasks彼此独立，并在不同的线程中执行，甚至在不同的机器或容器上执行。operator subtasks的数量为该特定操作（operator）的并行度（parallelism）。 流的并行度总是取决于它的生产操作。同一程序的不同操作可能具有不同级别的并行度。流可以在两个操作符之间以一对一 one to one（或转发 forwarding）模式传输数据，也可以采用重分布* Redistributing* 模式： one to one，如上图的source和map操作，保持数据元素的分区和顺序。 redistrubition，重分布。如上图中的 map() 和 keyBy/window，以及 keyBy/window 和 Sink，会改变流的分区。每个操作子任务根据具体的transfermation operator向不同的目标子任务发送数据。例如keyBy()(通过散列键重新分区)、broadcast()或rebalance()(随机重新分区)。在重分发交换中，关于不同键的聚合结果到达接收器的顺序是不确定的。 窗口（Windows） 窗口（Windows）概念是用于在流计算过程中划分聚合计算对应的事件范围，例如“count over the last 5 minutes”或者“sum of the last 100 elements”。窗口可以是时间驱动的（例如:每30秒），也可以是数据驱动的（例如:每100个元素）。窗口可以分为以下三类：翻滚窗口 tumbling windows（没有重叠）、滑动窗口 sliding windows（有重叠）和会话窗口 session windows（中间有一个不活动的间隙）。 时间（Time） 在流处理程序中提到时间（例如定义 windows）时，可以指不同的时间概念： 事件时间（Event Time） 是创建事件的时间。它通常由事件中的时间戳描述，例如由生产传感器或生产服务附加的时间戳。Flink通过时间戳分配程序timestamp assigners访问事件时间戳。 摄入时间 （Ingestion time） 是事件在源操作符处进入Flink数据流的时间。 处理时间（Processing Time） 是指每个基于时间的操作符执行的本地时间。 有状态操作（Stateful Operations）Flink数据流（dataflow）中有些操作只需要每次独立的处理一个事件，例如Event解析操作；有些操作需要记录多个事件的信息，例如窗口操作，后者即被称为有状态的操作（Stateful Operations）。 在Flink架构体系中，有状态计算可以说是Flink非常重要的特性之一。有状态计算是指在程序计算过程中，在Flink程序内部存储计算产生的中间结果，并提供给后续Function或算子计算结果使用。状态数据可以维系在本地存储中，这里的存储可以是Flink的堆内存或者堆外内存，也可以借助第三方的存储介质，例如Flink中已经实现的RocksDB，当然用户也可以自己实现相应的缓存系统去存储状态信息，以完成更加复杂的计算逻辑。和状态计算不同的是，无状态计算不会存储计算过程中产生的结果，也不会将结果用于下一步计算过程中，程序只会在当前的计算流程中实行计算，计算完成就输出结果，然后下一条数据接入，然后再处理。无状态计算实现的复杂度相对较低，实现起来比较容易，但是无法完成提到的比较复杂的业务场景，例如下面的例子： 我们需要输出每个传感器的每个采集值（数值），但是传感器的数据波动很大，如何对数据处理，使其波动没有那么大。这就需要当前的采集值需要联系之前的采集值，换而言之，就需要知道当前传感器的状态。 用户想按照分钟，小时、天进行聚合计算，求取当前的最大值、均值等聚会指标，这就需要利用状态来维护当前计算过程中产生的结果，例如事件的总数，总和以及最大，最小值。 具体内容后面会开一篇文章细讲 容错-CheckpointsFlink使用流重放和检查点的组合实现容错。检查点与每个输入流中的特定点以及每个操作符的对应状态相关。通过恢复运算符的状态并从检查点重放事件，可以从检查点恢复流数据流，同时保持一致性（恰好一次处理语义）。 检查点间隔是在执行期间用恢复时间（需要重放的事件的数量）来折衷容错开销的手段。 批量流（Batch Streaming）Flink执行批处理程序作为流程序的特殊情况，其中流是有界的（有限数量的元素）。 DataSet在内部被视为数据流。因此，上述概念以相同的方式应用于批处理程序，并且它们适用于流程序，除了少数例外： 批处理程序的容错不使用检查点。通过完全重放流来进行恢复。这是可能的，因为输入有限。这会使成本更多地用于恢复，但使常规处理更便宜，因为它避免了检查点。 DataSet API中的有状态操作使用简化的内存/核外数据结构，而不是键/值索引。 DataSet API引入了特殊的同步（超级步骤）迭代，这些迭代只能在有界流上进行。","permalink":"http://renzehello.github.io/2020/01/17/flink-programming-model/","photos":[]},{"tags":[{"name":"源代码","slug":"源代码","permalink":"http://renzehello.github.io/tags/%E6%BA%90%E4%BB%A3%E7%A0%81/"},{"name":"flink","slug":"flink","permalink":"http://renzehello.github.io/tags/flink/"}],"title":"Flink源码分析（一）：源码环境搭建及模块介绍","date":"2020/01/16","text":"更新至Flink release-1.9.0版本 环境 Java 1.8.0_191 Apache Maven 3.6.1 IntelliJ IDEA 2019.2.1 下载访问Apache Flink Github fork 代码到自己github 克隆代码 1git clone https://github.com/apache/flink.git 切换至 release-1.9 分支： 1git checkout release-1.9 编译 修改maven setting文件1234567 nexus-aliyun *,!jeecg,!jeecg-snapshots,!mapr-releases Nexus aliyun http://maven.aliyun.com/nexus/content/groups/public 编译1mvn clean package -DskipTests 项目模块Flink release-1.9 分支，Flink 主要模块： flink-annotations 注解； flink-clients 客户端； flink-connectors Flink 连接器，包括 Kafka、ElasticSearch、Cassandra、HBase、HDFS、Hive、JDBC 等； flink-container 提供对 Docker 和 Flink on Kubernetes 支持； flink-contrib 新模块准备或孵化区域； flink-core Flink 核心代码； flink-dist 提供对分发包支持； flink-filesystems 提供对文件系统的支持，包括 HDFS、S3 等； flink-formats 提供对文件格式的支持，包括 Avro、Parquet、JSON、CSV 等； flink-java Flink 底层 API； flink-libraries 提供对事件处理（Flink CEP）、图处理（Flink Gelly）和状态处理的支持； flink-mesos 提供对 Flink on Mesos 支持； flink-metrics 提供对监控的支持，包括 Graphite、InfluxDB、Prometheus、JMX、SLF4j 等； flink-ml-parent 提供对机器学习的支持； flink-optimizer Flink 优化器； flink-python 提供对 Python 的支持； flink-queryable-state 提供对 Queryable State 支持； flink-quickstart 提供对 Java 和 Scala 工程模板的支持； flink-runtime Flink 运行时； flink-runtime-web Dashboard UI； flink-scala 提供对 Scala 的支持； flink-scala-shell 提供对 Scala Shell 的支持； flink-shaded-curator 提供 Apache Curator 依赖的 shaded 包； flink-state-backends 提供对 RocksDB 状态后端的支持； flink-streaming-java DataStream API； flink-streaming-scala DataStream API 的 Scala 版； flink-table Table API 和 SQL； flink-yarn 提供对 Flink on YARN 支持。 以上模块中，粗体为重点模块，将在之后 Flink 源码分析的博文中详细介绍。 参考 Flink 源码分析（一）：源码环境搭建","permalink":"http://renzehello.github.io/2020/01/16/compile-code/","photos":[]},{"tags":[{"name":"源代码","slug":"源代码","permalink":"http://renzehello.github.io/tags/%E6%BA%90%E4%BB%A3%E7%A0%81/"}],"title":"如何高效的阅读源代码","date":"2020/01/16","text":"如何高效的阅读源代码随着数据工程的发展，应用于不同场景的大数据组件种类层出不穷。对于一种大数据工具或者组件的学习，能够在生产环境下使用，自然是最好的一种学习方式。除此之外，在熟练使用的基础上，对源代码进行阅读，不仅有助于加深当前一种工具的理解，更能帮助大家增加分布式、数据密集系统的知识储备，能够起到触类旁通的效果。 个人谈谈阅读源代码的经验。 第一阶段学习基本使用和基本原理，从应用角度对工具进行了解和学习 这是第一个阶段，首先从学习使用工具开始，从应用层面，对其进行一定了解，比如你可以使用shell-cli或者api进行操作。接下来可以尝试了解它的内部原理，注意，不需要通过阅读源代码了解内部原理，只需看一些博客、书籍，不仅需要知道它的基本架构以及各个模块的功能，而且应该知道其具体的工作流程及工作原理，可以自己在纸上完整画完架构及工作流程，越详细越好。 在这个阶段，建议你多看一些知名博客。如果你有实际项目驱动，那是再好不过了，理论联系实际是最好的学习方法；如果你没有项目驱动，那建议你不要自己一个人闷头学，多跟别人交流，多主动给别人讲讲，最好的学习方式还是“讲给别人听”。 第二阶段从无到入门，开始阅读hadoop源代码 这个阶段是最困苦和漫长的，尤其对于那些没有任何分布式经验的人。 很多人这个阶段没有走完，就放弃了，最后停留在应用层面。 当你把源代码导入eclipse或intellij idea，沏上一杯茶，开始准备优哉游哉地看源代码时，你懵逼了：你展开那数不尽的package和class，觉得无从下手，好不容易找到了入口点，然后你屁颠屁颠地通过eclipse的查找引用功能，顺着类的调用关系一层层找下去，最后迷失在了代码的海洋中，如同你在不尽的压栈，最后栈溢出了，你忘记在最初的位置。很多人经历过上面的过程，最后没有顺利逃出来，而放弃。 如果你正在经历这个过程，我的经验如下： 你需要选择一个稳定的release版本 (可以是最新版本，也可以是任一稳定版本)，在本地将源代码编译打包成功，因为在阅读源代码的过程中，一个随时可以debug的环境，可以使你理解代码的过程更加高效。 其次，需要了解工程的代码模块，知道每一个模块对应的作用，并在阅读源代码过程中，时刻谨记你当前阅读的代码属于哪一个模块，会在哪个组件中执行。之后你需要摸清各个组件的交互协议，也就是分布式中的RPC，你需要对RPC的使用方式有所了解，然后看各模块间的RPC protocol，到此，你把握了系统的骨架，这是接下来阅读源代码的基础； 接着，你要选择一个模块开始阅读，一般选择Client，这个模块相对简单些，会给自己增加信心，为了在阅读代码过程中，不至于迷失自己，建议在纸上画出类的调用关系，边看边画，加深理解。 在这个阶段，建议大家多看一些源代码分析博客和书籍，比如《xxxx术内幕》系列丛书。借助这些博客和书籍，你可以在前人的帮助下，更快地学习源代码，节省大量时间，注意，目前博客和书籍很多，建议大家广泛收集资料，找出最适合自己的参考资料。 这个阶段最终达到的目的，是对源代码整体架构和局部的很多细节，有了一定的了解。以hadoop为例，比如你知道MapReduce Scheduler是怎样实现的，MapReduce shuffle过程中，map端做了哪些事情，reduce端做了哪些事情，是如何实现的，等等。这个阶段完成后，当你遇到问题或者困惑点时，可以迅速地在Hadoop源代码中定位相关的类和具体的函数，通过阅读源代码解决问题，这时候，hadoop源代码变成了你解决问题的参考书。 第三个阶段根据需求，修改源代码。 这个阶段，是验证你阅读源代码成效的时候。你根据leader给你的需求，修改相关代码完成功能模块的开发。在修改源代码过程中，你发现之前阅读源代码仍过于粗糙，这时候你再进一步深入阅读相关代码，弥补第二个阶段中薄弱的部分。当然，很多人不需要经历第三个阶段，仅仅第二阶段就够了：一来能够通过阅读代码解决自己长久以来的技术困惑，满足自己的好奇心，二来从根源上解决解决自己遇到的各种问题。 这个阶段，没有太多的参考书籍或者博客，多跟周围的同事交流，通过代码review和测试，证明自己的正确性。 阅读源代码的目的不一定非是工作的需要，你可以把他看成一种修养，通过阅读源代码，加深自己对分布式系统的理解，培养自己踏实做事的心态。 参考 董的博客","permalink":"http://renzehello.github.io/2020/01/16/how-read-code/","photos":[]},{"tags":[],"title":"page","date":"2020/01/13","text":"","permalink":"http://renzehello.github.io/2020/01/13/page/","photos":[]},{"tags":[{"name":"testTag","slug":"testTag","permalink":"http://renzehello.github.io/tags/testTag/"}],"title":"post","date":"2020/01/13","text":"","permalink":"http://renzehello.github.io/2020/01/13/post/","photos":[]}]}