{"meta":{"title":"RZ","subtitle":"","description":"","author":"黑泽暗","url":"http://renzehello.github.io","root":"/"},"posts":[{"tags":[{"name":"bitmap, data structure","slug":"bitmap-data-structure","permalink":"http://renzehello.github.io/tags/bitmap-data-structure/"}],"title":"bitmap详解","date":"2020/01/18","text":"","permalink":"http://renzehello.github.io/2020/01/18/bitmap/","photos":[]},{"tags":[],"title":"image","date":"2020/01/18","text":"插入图片test1 test2 test3 test4官方办法 test5 test6 test7","permalink":"http://renzehello.github.io/2020/01/18/image/","photos":[]},{"tags":[{"name":"flink","slug":"flink","permalink":"http://renzehello.github.io/tags/flink/"}],"title":"Flink编程模型","date":"2020/01/17","text":"抽象级别Flink提供了不同的抽象级别来开发流/批处理程序。 最低级别抽象只提供有状态流（stateful streaming）。它将Process Function嵌入到DataStream API中。它允许用户自由处理来自一个或多个流的事件，并使用一致的容错状态。此外，用户可以注册事件时间和处理时间回调，允许程序实现复杂的计算。 实际上，一般应用并不需要stateful streaming这么底层的抽象，而是采用Core Apis即可，例如DataStrean API（有界/无界的流）和DataSet API（有界的数据集）。这些常用的APIs提供了常见的数据处理构建块（building blocks for data processing），比如用户指定的各种形式的转换、连接、聚合、窗口、状态等。在这些api中处理的数据类型用各自的编程语言表示为类。 Table API 是一个以数据表为中心的声明性DSL，表可以(在表示流时)动态地更改表。Table API遵循(扩展的)关系模型:表有一个附加的schema(类似于关系数据库中的表)，而API提供了类似的操作，如select、project、join、group-by、aggregate等。虽然可以通过UDF进行拓展，但是Table Apis的表现力仍然要比Core Apis要弱，不过它使用起来更简洁。此外Table Api在执行之前会经过规则优化器的优化。Table Apis可以在tables和DataStream/DataSet之间自由转换，允许Table API和DataStream/DataSet API混合使用。这些都很像spark的接口类型。 Flink提供的最高级抽象是SQL。这种抽象在语义和表达性上都类似于Table API，但将程序表示为SQL查询表达式。SQL抽象与Table API紧密交互，SQL查询可以在Table API中定义的表上执行。 这里的接口抽象和spark很像，过后可以在没种抽象下加一段示例代码 程序和数据流 结果上图来看，Flink程序可以视为一个有向无环图，图的起点是data source，节点是各种transfermation operator，边就是streaming，终点是data sink。流（streams）和转换（transformations）是构成Flink程序的基本构建块（The basic building blocks，基本组成）。需要注意的是，Flink的DataSet API也是基于流实现的——稍后会详细介绍。从概念上讲，流是一种（可能永远不会结束）流动的数据记录，而转换是一种操作，它接收一个或者多个流作为输入，并且生产出一个或多个输出流作为结果。当执行时，Flink程序被影射到流式数据流（streaming dataflows），由流和转换操作组成。每个数据流开始于一个或多个数据源（sources），并结束于一个或多个接收器（sinks）。数据流类似于任意有向无环图（directed acyclic graphs DAGs）。 并行数据流 从并行和分布式的视角看Flink程序的话，流–>流分区；转换操作–>操作子任务抽象视角 | 并行视角 | 分类|-|-|-stream | stream partitions | - |transfermation operator | operator subtasks | one to one / redistribution | Flink程序是并行和分布式的。在执行期间，一个流有一个或多个流分区（stream partitions），每个操作符有一个或多个操作子任务（operator subtasks）。operator subtasks彼此独立，并在不同的线程中执行，甚至在不同的机器或容器上执行。operator subtasks的数量为该特定操作（operator）的并行度（parallelism）。 流的并行度总是取决于它的生产操作。同一程序的不同操作可能具有不同级别的并行度。流可以在两个操作符之间以一对一 one to one（或转发 forwarding）模式传输数据，也可以采用重分布* Redistributing* 模式： one to one，如上图的source和map操作，保持数据元素的分区和顺序。 redistrubition，重分布。如上图中的 map() 和 keyBy/window，以及 keyBy/window 和 Sink，会改变流的分区。每个操作子任务根据具体的transfermation operator向不同的目标子任务发送数据。例如keyBy()(通过散列键重新分区)、broadcast()或rebalance()(随机重新分区)。在重分发交换中，关于不同键的聚合结果到达接收器的顺序是不确定的。 窗口（Windows） 窗口（Windows）概念是用于在流计算过程中划分聚合计算对应的事件范围，例如“count over the last 5 minutes”或者“sum of the last 100 elements”。窗口可以是时间驱动的（例如:每30秒），也可以是数据驱动的（例如:每100个元素）。窗口可以分为以下三类：翻滚窗口 tumbling windows（没有重叠）、滑动窗口 sliding windows（有重叠）和会话窗口 session windows（中间有一个不活动的间隙）。 时间（Time） 在流处理程序中提到时间（例如定义 windows）时，可以指不同的时间概念： 事件时间（Event Time） 是创建事件的时间。它通常由事件中的时间戳描述，例如由生产传感器或生产服务附加的时间戳。Flink通过时间戳分配程序timestamp assigners访问事件时间戳。 摄入时间 （Ingestion time） 是事件在源操作符处进入Flink数据流的时间。 处理时间（Processing Time） 是指每个基于时间的操作符执行的本地时间。 有状态操作（Stateful Operations）Flink数据流（dataflow）中有些操作只需要每次独立的处理一个事件，例如Event解析操作；有些操作需要记录多个事件的信息，例如窗口操作，后者即被称为有状态的操作（Stateful Operations）。 在Flink架构体系中，有状态计算可以说是Flink非常重要的特性之一。有状态计算是指在程序计算过程中，在Flink程序内部存储计算产生的中间结果，并提供给后续Function或算子计算结果使用。状态数据可以维系在本地存储中，这里的存储可以是Flink的堆内存或者堆外内存，也可以借助第三方的存储介质，例如Flink中已经实现的RocksDB，当然用户也可以自己实现相应的缓存系统去存储状态信息，以完成更加复杂的计算逻辑。和状态计算不同的是，无状态计算不会存储计算过程中产生的结果，也不会将结果用于下一步计算过程中，程序只会在当前的计算流程中实行计算，计算完成就输出结果，然后下一条数据接入，然后再处理。无状态计算实现的复杂度相对较低，实现起来比较容易，但是无法完成提到的比较复杂的业务场景，例如下面的例子： 我们需要输出每个传感器的每个采集值（数值），但是传感器的数据波动很大，如何对数据处理，使其波动没有那么大。这就需要当前的采集值需要联系之前的采集值，换而言之，就需要知道当前传感器的状态。 用户想按照分钟，小时、天进行聚合计算，求取当前的最大值、均值等聚会指标，这就需要利用状态来维护当前计算过程中产生的结果，例如事件的总数，总和以及最大，最小值。 具体内容后面会开一篇文章细讲 容错-CheckpointsFlink使用流重放和检查点的组合实现容错。检查点与每个输入流中的特定点以及每个操作符的对应状态相关。通过恢复运算符的状态并从检查点重放事件，可以从检查点恢复流数据流，同时保持一致性（恰好一次处理语义）。 检查点间隔是在执行期间用恢复时间（需要重放的事件的数量）来折衷容错开销的手段。 批量流（Batch Streaming）Flink执行批处理程序作为流程序的特殊情况，其中流是有界的（有限数量的元素）。 DataSet在内部被视为数据流。因此，上述概念以相同的方式应用于批处理程序，并且它们适用于流程序，除了少数例外： 批处理程序的容错不使用检查点。通过完全重放流来进行恢复。这是可能的，因为输入有限。这会使成本更多地用于恢复，但使常规处理更便宜，因为它避免了检查点。 DataSet API中的有状态操作使用简化的内存/核外数据结构，而不是键/值索引。 DataSet API引入了特殊的同步（超级步骤）迭代，这些迭代只能在有界流上进行。","permalink":"http://renzehello.github.io/2020/01/17/flink-programming-model/","photos":[]},{"tags":[{"name":"源代码","slug":"源代码","permalink":"http://renzehello.github.io/tags/%E6%BA%90%E4%BB%A3%E7%A0%81/"},{"name":"flink","slug":"flink","permalink":"http://renzehello.github.io/tags/flink/"}],"title":"Flink源码分析（一）：源码环境搭建及模块介绍","date":"2020/01/16","text":"更新至Flink release-1.9.0版本 环境 Java 1.8.0_191 Apache Maven 3.6.1 IntelliJ IDEA 2019.2.1 下载访问Apache Flink Github fork 代码到自己github 克隆代码 1git clone https://github.com/apache/flink.git 切换至 release-1.9 分支： 1git checkout release-1.9 编译 修改maven setting文件1234567 nexus-aliyun *,!jeecg,!jeecg-snapshots,!mapr-releases Nexus aliyun http://maven.aliyun.com/nexus/content/groups/public 编译1mvn clean package -DskipTests 项目模块Flink release-1.9 分支，Flink 主要模块： flink-annotations 注解； flink-clients 客户端； flink-connectors Flink 连接器，包括 Kafka、ElasticSearch、Cassandra、HBase、HDFS、Hive、JDBC 等； flink-container 提供对 Docker 和 Flink on Kubernetes 支持； flink-contrib 新模块准备或孵化区域； flink-core Flink 核心代码； flink-dist 提供对分发包支持； flink-filesystems 提供对文件系统的支持，包括 HDFS、S3 等； flink-formats 提供对文件格式的支持，包括 Avro、Parquet、JSON、CSV 等； flink-java Flink 底层 API； flink-libraries 提供对事件处理（Flink CEP）、图处理（Flink Gelly）和状态处理的支持； flink-mesos 提供对 Flink on Mesos 支持； flink-metrics 提供对监控的支持，包括 Graphite、InfluxDB、Prometheus、JMX、SLF4j 等； flink-ml-parent 提供对机器学习的支持； flink-optimizer Flink 优化器； flink-python 提供对 Python 的支持； flink-queryable-state 提供对 Queryable State 支持； flink-quickstart 提供对 Java 和 Scala 工程模板的支持； flink-runtime Flink 运行时； flink-runtime-web Dashboard UI； flink-scala 提供对 Scala 的支持； flink-scala-shell 提供对 Scala Shell 的支持； flink-shaded-curator 提供 Apache Curator 依赖的 shaded 包； flink-state-backends 提供对 RocksDB 状态后端的支持； flink-streaming-java DataStream API； flink-streaming-scala DataStream API 的 Scala 版； flink-table Table API 和 SQL； flink-yarn 提供对 Flink on YARN 支持。 以上模块中，粗体为重点模块，将在之后 Flink 源码分析的博文中详细介绍。 参考 Flink 源码分析（一）：源码环境搭建","permalink":"http://renzehello.github.io/2020/01/16/compile-code/","photos":[]},{"tags":[{"name":"源代码","slug":"源代码","permalink":"http://renzehello.github.io/tags/%E6%BA%90%E4%BB%A3%E7%A0%81/"}],"title":"如何高效的阅读源代码","date":"2020/01/16","text":"如何高效的阅读源代码随着数据工程的发展，应用于不同场景的大数据组件种类层出不穷。对于一种大数据工具或者组件的学习，能够在生产环境下使用，自然是最好的一种学习方式。除此之外，在熟练使用的基础上，对源代码进行阅读，不仅有助于加深当前一种工具的理解，更能帮助大家增加分布式、数据密集系统的知识储备，能够起到触类旁通的效果。 个人谈谈阅读源代码的经验。 第一阶段学习基本使用和基本原理，从应用角度对工具进行了解和学习 这是第一个阶段，首先从学习使用工具开始，从应用层面，对其进行一定了解，比如你可以使用shell-cli或者api进行操作。接下来可以尝试了解它的内部原理，注意，不需要通过阅读源代码了解内部原理，只需看一些博客、书籍，不仅需要知道它的基本架构以及各个模块的功能，而且应该知道其具体的工作流程及工作原理，可以自己在纸上完整画完架构及工作流程，越详细越好。 在这个阶段，建议你多看一些知名博客。如果你有实际项目驱动，那是再好不过了，理论联系实际是最好的学习方法；如果你没有项目驱动，那建议你不要自己一个人闷头学，多跟别人交流，多主动给别人讲讲，最好的学习方式还是“讲给别人听”。 第二阶段从无到入门，开始阅读hadoop源代码 这个阶段是最困苦和漫长的，尤其对于那些没有任何分布式经验的人。 很多人这个阶段没有走完，就放弃了，最后停留在应用层面。 当你把源代码导入eclipse或intellij idea，沏上一杯茶，开始准备优哉游哉地看源代码时，你懵逼了：你展开那数不尽的package和class，觉得无从下手，好不容易找到了入口点，然后你屁颠屁颠地通过eclipse的查找引用功能，顺着类的调用关系一层层找下去，最后迷失在了代码的海洋中，如同你在不尽的压栈，最后栈溢出了，你忘记在最初的位置。很多人经历过上面的过程，最后没有顺利逃出来，而放弃。 如果你正在经历这个过程，我的经验如下： 你需要选择一个稳定的release版本 (可以是最新版本，也可以是任一稳定版本)，在本地将源代码编译打包成功，因为在阅读源代码的过程中，一个随时可以debug的环境，可以使你理解代码的过程更加高效。 其次，需要了解工程的代码模块，知道每一个模块对应的作用，并在阅读源代码过程中，时刻谨记你当前阅读的代码属于哪一个模块，会在哪个组件中执行。之后你需要摸清各个组件的交互协议，也就是分布式中的RPC，你需要对RPC的使用方式有所了解，然后看各模块间的RPC protocol，到此，你把握了系统的骨架，这是接下来阅读源代码的基础； 接着，你要选择一个模块开始阅读，一般选择Client，这个模块相对简单些，会给自己增加信心，为了在阅读代码过程中，不至于迷失自己，建议在纸上画出类的调用关系，边看边画，加深理解。 在这个阶段，建议大家多看一些源代码分析博客和书籍，比如《xxxx术内幕》系列丛书。借助这些博客和书籍，你可以在前人的帮助下，更快地学习源代码，节省大量时间，注意，目前博客和书籍很多，建议大家广泛收集资料，找出最适合自己的参考资料。 这个阶段最终达到的目的，是对源代码整体架构和局部的很多细节，有了一定的了解。以hadoop为例，比如你知道MapReduce Scheduler是怎样实现的，MapReduce shuffle过程中，map端做了哪些事情，reduce端做了哪些事情，是如何实现的，等等。这个阶段完成后，当你遇到问题或者困惑点时，可以迅速地在Hadoop源代码中定位相关的类和具体的函数，通过阅读源代码解决问题，这时候，hadoop源代码变成了你解决问题的参考书。 第三个阶段根据需求，修改源代码。 这个阶段，是验证你阅读源代码成效的时候。你根据leader给你的需求，修改相关代码完成功能模块的开发。在修改源代码过程中，你发现之前阅读源代码仍过于粗糙，这时候你再进一步深入阅读相关代码，弥补第二个阶段中薄弱的部分。当然，很多人不需要经历第三个阶段，仅仅第二阶段就够了：一来能够通过阅读代码解决自己长久以来的技术困惑，满足自己的好奇心，二来从根源上解决解决自己遇到的各种问题。 这个阶段，没有太多的参考书籍或者博客，多跟周围的同事交流，通过代码review和测试，证明自己的正确性。 阅读源代码的目的不一定非是工作的需要，你可以把他看成一种修养，通过阅读源代码，加深自己对分布式系统的理解，培养自己踏实做事的心态。 参考 董的博客","permalink":"http://renzehello.github.io/2020/01/16/how-read-code/","photos":[]},{"tags":[],"title":"page","date":"2020/01/13","text":"","permalink":"http://renzehello.github.io/2020/01/13/page/","photos":[]},{"tags":[{"name":"testTag","slug":"testTag","permalink":"http://renzehello.github.io/tags/testTag/"}],"title":"post","date":"2020/01/13","text":"","permalink":"http://renzehello.github.io/2020/01/13/post/","photos":[]}]}