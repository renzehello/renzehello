{"meta":{"title":"RZ","subtitle":"","description":"","author":"黑泽暗","url":"http://renzehello.github.io","root":"/"},"posts":[{"tags":[{"name":"Flink","slug":"Flink","permalink":"http://renzehello.github.io/tags/Flink/"}],"title":"Flink时间和水印","date":"2020/02/05","text":"Event Time / Processing Time / Ingestion Time Flink支持三种时间概念： Event time（事件时间） 事件生成的时间，即数据产生时自带时间戳。 必须指定水位的生成方式 优势：理想情况下可以保证确定性，数据延迟、乱序、重放都可以保证处理结果一致性 缺点：实际生产中，在处理非严格有序的场景下需要产生延迟，由于不能无限制等待，一定程度上会限制确定性。有时当事件时间程序实时处理实时数据时，它们将使用一些处理时间操作，以确保它们及时进行。 Ingestion time（摄入时间） 事件进入flink的时间 与事件时间相比，摄入时间程序不能处理任何无序事件或延迟数据，但程序不必指定如何生成Watermarks，它自动进行时间戳分配和自动Watermarks生成。 与处理时间相比，确定性更高一些。 Processing time（处理时间） 每一个执行window操作的机器的系统时间。当流程序在处理时间上运行时，所有基于时间的操作(如时间窗口)将使用运行各自操作符的机器的系统时间。 优势：低延迟、高性能，不需要流和机器之间的协调。 缺点：不确定性。会受到event产生的速度、到达flink的速度、在算子之间传输速度等因素的影响，压根就不管顺序和延迟。 比较 性能： ProcessingTime> IngestTime> EventTime延迟： ProcessingTime< IngestTime< EventTime确定性： EventTime> IngestTime> ProcessingTime是否设置水位：Event time必须指定；Ingestion time自动水位生成；Processing time不需要指定。 根据业务选择最合适的时间Hadoop的日志进入Flink的时间为2018-12-23 17:43:46,666（Ingest Time），在进入window操作时那台机器的系统时间是2018-12-23 17:43:47,120（Processing Time），日志的具体内容是：（Event Time）2018-12-23 16:37:15,624 INFOorg.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider - Failing over to rm2 要统计每个5min内的日志error个数，哪个时间是最有意义的? 最佳选择就是event time。一般都需要使用event time，除非由于特殊情况只能用另外两种时间来代替。 事件-时间（Event-Time）处理在“时间语义”中，我们解释了处理时间与事件时间的不同点。处理时间较好理解，因为它基于本地机器的时间，它产生的是有点任意的、不一致的、以及无法复现的结果。而事件时间的语义产生的是可复现的、一致性的结果，它对于很多流处理场景是一个硬性的要求。然而，相对于处理时间语义，事件时间语义应用需要额外的配置，并且引入了更多的系统内部构件。 时间戳（Timestamp）所有由Flink 事件-时间流应用生成的条目都必须伴随着一个时间戳。时间戳将一个条目与一个特定的时间点关联起来，一般这个时间点表示的是这条record发生的时间。不过application可以随意选择时间戳的含义，只要流中条目的时间戳是随着流的前进而递增即可。 水印（Watermarks）{% asset_img times_clocks.svg times_clocks %} 解决目标： out-of-order/late element （乱序和延迟事件）实时系统中，由于各种原因造成的延时，造成某些消息发到flink的时间延时于事件产生的时间。如果基于event time构建window，但是对于late element，我们又不能无限期的等下去，必须要有个机制来保证一个特定的时间后，必须触发window去进行计算了。这个特别的机制，就是watermark。在一个事件-时间应用中，水印用于从每个task中获取当前的事件时间。Time-based operators 使用这个时间触发计算，并取得进展。例如，一个time-window 任务在到达window的结束边界后，会触发计算并产生输出。在Flink中，水印是以特殊的records实现的，这些records会持有一个Long类型的值，作为时间戳。如下图所示，水印流记录穿插在正常流记录（包含时间戳）之中： {% asset_img watermarks.png watermarks %} {% asset_img times_clocks.svg times_clocks %} 水印有如下特征： 是event time处理进度的标志。它们必须单调递增，以确保任务的event-time时钟向前推进，而不是向后。 它们与记录的时间戳是相关的。一个时间戳为T的水印表示的是：在它之后接下来的所有记录的时间戳，都必须大于T。官网的表达是：表示比watermark更早(更老)的事件都已经到达(没有比水位线更低的数据 )。 基于watermark来进行窗口触发计算的判断。有序流中的watermarks在某些情况下，基于Event Time的数据流是有续的(相对event time)。在有序流中，watermark就是一个简单的周期性标记。{% asset_img inorder-watermarks.png inorder-watermarks %} 乱序流中的watermarks在更多场景下，基于Event Time的数据流是无续的(相对event time)。在无序流中，watermark至关重要，它告诉operator比watermark更早(更老/时间戳更小)的事件已经到达， operator可以将内部事件时间提前到watermark的时间戳(可以触发window计算啦){% asset_img outorder-watermarks.png outorder-watermarks %} 并行流中的watermarks{% asset_img par-watermarks.png par-watermarks.png %} 通常情况下， watermark在source函数中生成，但是也可以在source后任何阶段，如果指定多次 watermark，后面指定的 watermark会覆盖前面的值。 source的每个sub task独立生成水印。watermark通过operator时会推进operators处的当前event time，同时operators会为下游生成一个新的watermark。多输入operator(union、 keyBy、 partition)的当前event time是其输入流event time的最小值。 Timestamp/Watermark两种生成方式只有基于EventTime的流处理程序需要指定Timestamp和Watermarks的生成方式。指定时间特性为Event Time(前面讲过)。 final StreamExecutionEnvironment env =StreamExecutionEnvironment.getExecutionEnvironment();env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); 为了让event time工作，Flink需要知道事件的时间戳，这意味着流中的每个元素都需要分配其事件时间戳。这个通常是通过抽取或者访问事件中某些字段的时间戳来获取的。时间戳的分配伴随着水印的生成，告诉系统事件时间中的进度。这里有两种方式来分配时间戳和生成水印: 直接在数据流源中进行。 通过timestamp assigner和watermark generator生成:在Flink中，timestamp分配器也定义了用来发射的水印。注意:timestamp和watermark都是通过从1970年1月1日0时0分0秒到现在的毫秒数来指定的。 带有Timestamp和Watermark的源函数(Source Function with Timestamps And Watermarks)数据流源可以直接为它们产生的数据元素分配timestamp，并且他们也能发送水印。这样做的话，就没必要再去定义timestamp分配器了，需要注意的是:如果一个timestamp分配器被使用的话，由源提供的任何timestamp和watermark都会被重写。为了通过源直接为一个元素分配一个timestamp，源需要调用SourceContext中的collectWithTimestamp(...)方法。为了生成watermark，源需要调用emitWatermark(Watermark)方法。下面是一个简单的(无checkpoint)由源分配timestamp和产生watermark的例子: override def run(ctx: SourceContext[MyType]): Unit = { while (/* condition */) { val next: MyType = getNext() ctx.collectWithTimestamp(next, next.eventTimestamp) if (next.hasWatermarkTime) { ctx.emitWatermark(new Watermark(next.getWatermarkTime)) } }} 时间戳分配器/水印生成器（Timestamp Assigners / Watermark Generators）Timestamp分配器获取一个流并生成一个新的带有Timestamp元素和水印的流。如果原始流已经有时间戳和/或水印，则Timestamp分配程序将覆盖它们。Timestamp分配器通常在数据源之后立即指定，但这并不是严格要求的。通常是在timestamp分配器之前先解析(MapFunction)和过滤(FilterFunction)。在任何情况下，都需要在事件时间上的第一个操作(例如第一个窗口操作)之前指定timestamp分配程序。有一个特殊情况，当使用Kafka作为流作业的数据源时，Flink允许在源内部指定timestamp分配器和watermark生成器。更多关于如何进行的信息请参考Kafka Connector的文档。接下来的部分展示了要创建自己的timestamp抽取器和watermark发射器，程序员需要实现的主要接口。想要查看Flink预定义的抽取器。 val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)val stream: DataStream[MyEvent] = env.readFile( myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100, FilePathFilter.createDefaultFilter())val withTimestampsAndWatermarks: DataStream[MyEvent] = stream .filter( _.severity == WARNING ) .assignTimestampsAndWatermarks(new MyTimestampsAndWatermarks())withTimestampsAndWatermarks .keyBy( _.getGroup ) .timeWindow(Time.seconds(10)) .reduce( (a, b) => a.add(b) ) .addSink(...) 水印类型周期性水印(With Periodic Watermarks)Periodic Watermarks使用步骤：1.基于Timer2.ExecutionConfig.setAutoWatermarkInterval(msec) (默认是 200ms, 设置watermarker发送的周期)。3.实现AssignerWithPeriodicWatermarks接口。这里我们展示了两个使用周期性水印生成的时间戳分配器的简单示例。请注意，Flink附带了一个BoundedOutOfOrdernessTimestampExtractor，类似于下面所示的BoundedOutOfOrdernessGenerator。 /** * This generator generates watermarks assuming that elements arrive out of order, * but only to a certain degree. The latest elements for a certain timestamp t will arrive * at most n milliseconds after the earliest elements for timestamp t. */class BoundedOutOfOrdernessGenerator extends AssignerWithPeriodicWatermarks[MyEvent] { val maxOutOfOrderness = 3500L // 3.5 seconds var currentMaxTimestamp: Long = _ override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = { val timestamp = element.getCreationTime() currentMaxTimestamp = max(timestamp, currentMaxTimestamp) timestamp } override def getCurrentWatermark(): Watermark = { // return the watermark as current highest timestamp minus the out-of-orderness bound new Watermark(currentMaxTimestamp - maxOutOfOrderness) }}/** * This generator generates watermarks that are lagging behind processing time by a fixed amount. * It assumes that elements arrive in Flink after a bounded delay. */class TimeLagWatermarkGenerator extends AssignerWithPeriodicWatermarks[MyEvent] { val maxTimeLag = 5000L // 5 seconds override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = { element.getCreationTime } override def getCurrentWatermark(): Watermark = { // return the watermark as current time minus the maximum time lag new Watermark(System.currentTimeMillis() - maxTimeLag) }} 带断点的水印(With Punctuated Watermarks)Puncuated WaterMarks1.基于某些事件触发watermark的生成和发送(由用户代码实现，例如遇到特殊元素) 。2.实现AssignerWithPeriodicWatermarks接口。间断性调用getCurrentWatermark，它会根据一个条件发送watermark，这个条件可以自己去定义。注意: 可以在每个事件上生成一个watermark。但是，由于每个watermark都会导致下游的一些计算，过多的watermark会降低性能。 class PunctuatedAssigner extends AssignerWithPunctuatedWatermarks[MyEvent] { override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long = { element.getCreationTime } override def checkAndGetNextWatermark(lastElement: MyEvent, extractedTimestamp: Long): Watermark = { if (lastElement.hasWatermarkMarker()) new Watermark(extractedTimestamp) else null }} 每个Kafka分区的Timestamp（TimeStamps per Kafka Partion）当使用Apache Kafka作为数据源时，每个Kafka分区可能有一个简单的事件时间模式(递增timestamp或有界的无序)。然而，当使用来自Kafka的流时，多个分区通常是并行使用的，将事件与分区交叉,破坏了每个分区的数据模型(这是Kafka消费者客户端所固有的工作方式)在这种情况下，您可以使用Flink支持Kafka-partition-aware生成水印。该特性可以在Kafka消费者内部生成watermarks，每个分区的watermarks合并方式与流shuffles时合并watermarks的方式相同。例如，如果事件时间戳严格按照Kafka分区递增排列，那么使用升序时间戳水印生成器生成每个分区的水印将产生完美的整体水印。下图展示了如何使用每个kafka分区生成水印，以及在这种情况下水印如何通过流数据传播。 val kafkaSource = new FlinkKafkaConsumer09[MyType](\"myTopic\", schema, props)kafkaSource.assignTimestampsAndWatermarks(new AscendingTimestampExtractor[MyType] { def extractAscendingTimestamp(element: MyType): Long = element.eventTimestamp})val stream: DataStream[MyType] = env.addSource(kafkaSource) {% asset_img kafka-watermarks.svg kafka-watermarks %} 预定义Timestamp Extractors / Watermark Emitters为了进一步简化此类任务的编程工作，Flink附带了一些预先实现的时间戳赋值器。 Assigners with ascending timestamps适用于event时间戳单调递增的场景，数据没有太多延时。请注意，只需要每个并行数据源任务的时间戳升序。例如，如果在特定设置中，一个并行数据源实例读取一个Kafka分区，则只需在每个Kafka分区内将时间戳升序。Flink的水印合并机制将在并行流被shuffle、union、join或merge时生成正确的水印。 DataStream stream = ...DataStream withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(new AscendingTimestampExtractor() { @Override public long extractAscendingTimestamp(MyEvent element) { return element.getCreationTime(); }}); 允许固定延迟的Assigner适用于预先知道最大延迟的场景(例如最多比之前的元素延迟3000ms)。 DataStream stream = ...DataStream withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor(Time.seconds(10)) { @Override public long extractTimestamp(MyEvent element) { return element.getCreationTime(); }}); Flink事件时间处理和水印实例理解参考博客：https://blog.csdn.net/a6822342/article/details/78064815 后续补充 引用 https://www.cnblogs.com/dajiangtai/p/10697318.html https://www.jianshu.com/p/e6c7957d76d9 https://blog.csdn.net/a6822342/article/details/78064815 http://vishnuviswanath.com/flink_eventtime.html Flink官网：https://ci.apache.org/projects/flink/flink-docs-release-1.9/zh/dev/event_timestamp_extractors.html","permalink":"http://renzehello.github.io/2020/02/05/event-time/","photos":[]},{"tags":[{"name":"Data Governance","slug":"Data-Governance","permalink":"http://renzehello.github.io/tags/Data-Governance/"}],"title":"元数据总线设计","date":"2020/01/21","text":"元数据是数据治理工作开展的基础，元数据读写操作的高效和准确是数据治理各个平台建设的保障。 19年下半年，对数据治理内部元数据API和元数据总线进行了一次深度改造。本次对元数据总线的系统架构、消息设计和关键流程进行分享。 概要 业务需求与挑战 元数据总线解决方案 迁移方案 未来规划 业务需求与挑战1. 旧版元数据总线问题 整体来看，旧版元数据是一种高度耦合的中心化设计。DDL被HiveServer Hook和IDP（集成开发平台）解析之后发送Kafka，由元数据系统消费并存储。数据治理内部的各平台（包括数据地图、资产中心、安全中心、质量中心）以及IDP可以通过rpc或者REST的方式请求元数据系统，对元数据进行读取和写入操作。这样的架构目前遇到以下问题： 数据完整性问题：HiveServer Hook和IDP难以覆盖所有场景的DDL操作。例如通过spark程序建表，通过beeline建表等。 数据一致性问题：如果细节放大图所示，在通过IDP可视化建表的时候，按照图中步骤依次执行。 IDP将DDL（创建表和列语句）发送到HiveServer， 由Hook解析DDL并发送到血缘系统 血缘系统处理之后将新建表和列的信息发送到Kafka 血缘系统将消息发送Kafka之后会通知IDP IDP发送业务元数据更新请求到Kafka 然而由于网络抖动，会偶发性地出现业务元数据更新消息比创建表和列消息先消费的情况，导致数据一致性问题出现。 耦合严重：元数据系统维护了很多与其他平台业务相关的元数据信息（如资产的业务负责人、技术负责人和生命周期，安全的权限负责人、安全级别等）。随着治理业务的扩展，元数据系统将会越发臃肿。 元数据来源扩展性问题：目前元数据来源类型主要是Hive信息，接入其他异构数据源，需要基于元数据系统提供定制的读写接口，扩展成本较高。 2. 业务目标一句话描述元数据API/元数据改造的目标：以元数据API/元数据总线为基础，按元数据类别拆分到不同服务提供读写服务化、消息同步等； 元数据总线解决方案1. 架构设计该架构图以hive为例 新的元数据总线架构主要有以下特点： 切换数据源：Hive元数据来源由HiveServer Hook&Idp切换为HiveServer MetaStore，从而保障了元数据完整性。 解耦元数据系统SDK。按照业务类别将元数据拆分为基础元数据、资产元数据、安全元数据和质检元数据。对应存储、读写和SDK由各自业务平台独立维护。 业务系统之间通信方式。元数据系统的解耦意味着需要一种更加灵活的通信方式，来保证各业务系统之间可以自由读取对方管理的元数据。旧版架构中，业务系统之间不直接进行通信，而是由元数据系统提供读写接口，业务系统只跟元数据系统进行通信。新的架构中，业务系统之间通过定制的消息格式，以元数据总线为通道实现消息通信。 主要模块功能概述： Basic Server / Asset Server / Security Server / Quality Server 可靠的相关元数据读写接口； 相关元数据的存储； 发送变更消息到元数据总线； Hive Level API 封装核心SDK，提供合并、订制接口； 兼容旧方案部分系统的功能； 元数据总线（Basem SDK） 提供统一的元数据变更消息定义； 提供公共元数据相关工具类（Urn） 2. 消息设计考虑到元数据间存在关联关系，项目内部均使用Java类进行描述与值传递，而与外部的数据传输（如：读写kafka、rpc接口），一般使用proto3进行。在此不会对元数据总线消息设计细节进行讲解，消息结构如下：BasemBusEvent字段 | 类型 | 说明 | 备注-|-|-from_biz | FromBiz | 事件来源op_type | OpType | 操作类型 | 更新/删除/xxxurn | string | urntimestamp | uint64 | 消息生产的时间戳event_package | one of | 消息体 | 具体参考实体定义 3. 关键流程创建Hive表 IDP是内部的数据集成开发平台，提供了可视化的工具辅助数据人员进行表创建工作。在创建Hive表的流程中，IDP会根据表元数据的业务类别拆分为基础元数据、资产元数据、安全元数据等类别： 新建表和列的基础元数据：在HiveServer对MetaStore进行解析，并发送到元数据总线，被基础元数据Server消费； 资产和安全等业务元数据：由IDP调用对应的业务系统Server执行存储逻辑。业务Server会对新建表的基础元数据是否存在进行校验，如果不存在会执行等待和超时处理策略；反之则会在业务系统内部执行元数据存储，并将元数据消息发送至元数据总线，继而被需要的其他业务系统所消费。解决旧版架构里数据一致性问题。 删除Hive表 删除表操作，首先通过生命周期系统对Hive表进行回收，Hive MetaStore Server将删除消息发送至Kafka被基础元数据系统消费，然后基础元数据系统会将删除消息发送至元数据总线，其他业务系统根据自身的需求对删除消息进行处理。 迁移方案元数据总线上线的时候涉及到一个问题，如何在各业务系统不停机的情况下，保证新旧版本元数据的一致性。具体而言，在上线的时候，如果简单的采用迁移历史全量数据->切换新系统这种二阶段串行同步数据的方式，必然会在一个时间窗口内丢失实时的元数据读写请求（增删改查）。因此需要制定一个平滑迁移的方案，保障数据的一致性和系统的可靠性，下面以资产中心迁移流程为例说明： 如图所示，资产中心迁移流程分为两个阶段依次执行： 基础库表列元数据迁移：需要保证基础库表列的变更与metastore一致。 暂停基础元数据Basis Server消费MetaStore发到Kafka的消息。 调用basis提供的接口，拉取旧元数据系统的全量数据（实际上是一份快照数据）。 启动Basis Server对数据总线的消费。 对比新旧数据同步情况。 在基础库表列信息迁移中，BasisServer消费的消息来自于Kafka，所以可以通过Kafka保存新增消息的方式，保证增量消息不丢失，从而理论上保障新旧数据的一致性。 资产业务元数据迁移：在基础库表列元数据迁移执行完成之后启动。 涉及到资产元数据的业务系统（如资产中心）启动双写。 IDP切换使用High level api。1和2执行之后，资产元数据的增量数据就已经开始往新旧表里同时写入，此时新表里只有增量数据，没有存量数据。 通过rpc接口，开始从旧版元数据系统sref迁移存量数据。由于新表中已经存在新增数据，因此迁移存量数据过程中，对于表主键已经存在的数据执行丢弃策略，这样保障了资产元数据是最新的。 对于新旧数据一致性，理论上不会有一致性问题。 切换度接口。 双写停止，就服务下线。 除了资产中心的迁移之外，其余业务平台的迁移过程与此类似。需要注意的是对于各业务系统之间存在的调用依赖，需要梳理上线顺序。 总线优势 SSOT单一数据源（Single source of truth），大家有兴趣可以google一下，好像是前端开发框架提出来的，总体遵循的原则： 1.取值只读2.存值通道唯一3.数据订阅4.存储值改变通知比如我们的gitlab就是一个单一代码源，我们的元数据总线通过分域来实现SSOT（元数据分域：技术、质检、血缘、资产、安全等）。现实中SSOT是比较难的，需要顶层架构设计，但一旦建立起来各模块就可以互相独立快速发展了。 未来规划目前元数据系统主要管理的是Hive的元数据，未来一定会接入更多类型的数据源，比如Druid、Kafka Topic等。 问题新版元数据总线的设计思路是一个拆分解耦的过程，各业务平台对自身设计的业务元数据进行管理。但是新方案并不是没有缺点： 对于元数据的调用方而言，原本只需要调用一次元数据系统，现在需要分别调用多个元数据业务系统。目前是通过封装High level api的方式，减少元数据系统使用方的迁移成本。 对于多条件复杂查询请求，如果是旧版的设计，则只需要在元数据系统的数据库中进行剪枝和join操作，而对于新版元数据系统，则需要在不同的业务系统中分别进行查询，并将结果聚合拼接。这样能够保证目前线上的使用场景，是一个疑问。 “分久必合，合久必分”。","permalink":"http://renzehello.github.io/2020/01/21/metadata-bus/","photos":[]},{"tags":[{"name":"Flink","slug":"Flink","permalink":"http://renzehello.github.io/tags/Flink/"}],"title":"Flink分布式运行时环境","date":"2020/01/19","text":"Tasks和操作链（Operator Chains） 在分布式执行的时候，Flink可以将多个operator subtasks合并成一个操作链（operator chains），即一个task（我理解这里的task应该不是指完整的一个计算流程，应该是以重分布operator为界进行划分的）。每个task由一个线程执行，这样做的好处是能够减少线程间切换和缓存的开销，增加整个系统的吞吐量。 上图实例中包含五个suntasks，对应五个并行的thread。注意结合flink编程模型中operator subtask和subtask的区别。operator subtask对应一个operaoter节点，subtask对应一个operatoer chain。 Job Managers, Task Managers, Clients Flink分布式运行时由两种进程组成： JobManager（或Master）：用于协调分布式执行。负责调度任务，检查点，失败恢复等。一般是一个JM，高可用架构下会有多个JM，但是只有一个运行，其余是standby。 TaskManager（或Worker）：负责具体的tasks执行（更准确地说，是计算子任务），并对数据流进行缓冲、交换。Flink 运行环境中至少包含一个任务管理器。 TM和JM可以运行在多种集群环境，例如standalone集群、容器、YARN等。TM会连接JM，并通报自己的健康情况，等待被分配任务。 Task Slots and Resources 每个 worker（TaskManager）都是一个独立的JVM进程，在独立线程里运行一个或更多的子任务。为了控制 worker 接收任务的数量，在 worker 中引入了任务槽（Task slots）的概念（每个 worker 中至少包含一个 slot）。每个 Task slot 代表 TaskManager 中一个固定的资源池子集。如果一个 TaskManager 有3个 slots，每个 slot 会分配其 1/3 的内存。将资源进行分槽可以让子任务避免同其他作业中的子任务竞争资源。注意，这里没有对 CPU 进行隔离；目前任务槽仅仅用于划分任务的内存。通过调整 Task slot 的数量，用户可以设定子任务之间独立运行的程度。如果 TaskManager 中只有一个槽，那么每个任务组都会在一个独立的JVM中运行。TaskManager 中配置更多的槽就意味着会有更多的子任务共享同一个 JVM。在同一个 JVM 中的任务会共享 TCP 连接（通过多路复用的方式）和心跳信息，同时他们也会共享数据集和数据结构，这在某种程度上可以降低单个任务的开销。 默认情况下，Flink允许来自不同task的subtask共享槽位，只要这些subtask属于同一个job。因此一个slot可以hold整个工作流。共享槽位有两个好处： Flink集群可以确保Task Slot个数与Job最高并行度一致，不需要计算task的具体数量。 提高资源利用率。如果没有槽共享机制，非密集型的 source/map() 子任务就会和密集型的 window 子任务一样阻塞大量资源。如果有任务槽共享机制，在程序的并发量从 2 提高到 6 的情况下（如下图），就可以让密集型子任务完全分散到任务管理器中，从而可以显著提高槽的资源利用率。说白了，槽共享机制就决定了每个槽位并不是只能运行某一类subtask，这样密集型的operator task就可以完全分派给所有的slot，从而增加并行度和资源使用效率。 Flink API 中包含一个 resource group 机制，可以避免不合理的任务槽共享。一般来说，默认的任务槽数量应设置为 CPU cores 的数量。 State Backends Flink中提供了StateBackend来存储和管理Checkpoint过程中的状态数据。Flink中一共实现了三种类型的状态管理器，包括基于内存的MemoryStateBackend，基于文件系统的FsStateBackend和基于RockDB作为存储介质的RocksDBStateBackend。这三种类型的StateBackend均能有效地存储Flink流式计算过程中产生的状态数据，默认情况下Flink使用的是内存作为状态管理器。除了定义保存状态的数据结构之外，StateBackend还实现了获取key/value状态的时间点snapshot的逻辑，并将该snapshot存储为Checkpoint的一部分。 Savepoints 使用DataStream API中编写的程序可以使用Savepoint恢复执行。Savepoint允许在不丢失任何状态的情况下更新程序和Flink集群。Savepoint是手动触发的Checkpoint，它获取程序的Snapshot并将其写入StateBackend。它们依赖于常规的Checkpoint机制。程序在执行过程中，会定期在工作节点上进行Snapshot并生成Checkpoint。对于恢复，只需要最后一个完成的Checkpoint，而之前的Checkpoint可以在新的Checkpoint完成后直接安全地丢弃掉。Savepoint类似于这些定期的Checkpoint，只是它们是由用户触发的并且在新Checkpoint完成时不会自动过期。可以从命令行或通过REST API在取消作业时创建Savepoint。","permalink":"http://renzehello.github.io/2020/01/19/flink-distributed-runtime/","photos":[]},{"tags":[{"name":"Data Governance","slug":"Data-Governance","permalink":"http://renzehello.github.io/tags/Data-Governance/"}],"title":"数据健康分项目总结","date":"2020/01/19","text":"数据健康分是由数据资产组在19年下半年推动的一个项目，健康分旨在通过一个分数来定量描述目前数据资产、各业务线在数据方面存在的问题情况，并通过分数各个组成部分来定位到数据问题所在的具体方面，进而推动各部门和数据负责人改进问题，提高数据资产价值。由此可见，并不能将健康分简单地描述为一个指标，在这个指标的指定规则后面，体现的是一整套数据资产价值的衡量体系，通过该体系组成的变化，可以有针对性地间接透出当前阶段数据治理的方向和目标。 背景当前阶段各业务线数据资产存在的主要问题： 元数据完整性低：无负责人、缺少必要描述信息。–数据找不到，数据不清楚 计算和存储资源浪费：僵尸表、无用表、没有生命周期管理 数据生产延迟、数据质量难以保证。希望通过将发现的问题归纳量化成健康分，通过分数反映目前数据资产、各个业务线在数据方面中存在问题，并通过各组成部分的分数映射到各个问题点，提醒相关人员和部门进行优化。 面向主体分为表、个人、一级部门。每个级别的主体，都会透出相应的健康分。 健康分计算规则不在此进行细节说明。 规划1. 方法论 发现问题 - 数据抽象 - 优化分析 - 推动优化 - 持续监控 2. 核心高热表元数据完善 介绍：梳理出核心和高热的数据重点进行元数据的补充完善，共计完成1113张表元数据补充完善。 持续：按周对数仓新增表元数据完善情况进行监控，及时邮件提醒数仓同学进行完善。 3. top表生命周期覆盖 介绍：对各业务线top表进行生命周期推进，目前共计5304张表覆盖生命周期，日均清理超过1.8PB数据空间(未计算副本数)，配合架构ec操作。目前除dp外其他业务线存储基本实现自循环 持续：根据数据最近的访问、依赖情况定期向用户推送生命周期设置和表下线建议。用户可以使用资产管理中心进行便捷安全的数据下线和清理操作。 4. 无主表认领 介绍：对各业务线的无主表开展有奖认领活动，共计认领19238张表(占比超过75%，目标60%)，其中下线表超过7155张表。 持续：目前像datasink等工具建表入口负责人信息收集基本已经收敛，spark等客户端建表入口负责人信息收集还需收敛 5. 运营和透出收益健康分的推动并非简单的制定规则->强行推动的过程，与数据治理其他项目一样，健康分项目也是一个重运营的事情，需要在运营主体主动接受的前提下，做到润物细无声的推动。因此除了制定规则，还需要根据实际情况提供工具手段、增加平台功能，减轻用户提高健康分的时间和人力成本。同时也需要配合运营活动，一方面透出健康分的成果和收益，一方面增加健康分主体的积极性。","permalink":"http://renzehello.github.io/2020/01/19/data-health-score/","photos":[]},{"tags":[{"name":"Bitmap","slug":"Bitmap","permalink":"http://renzehello.github.io/tags/Bitmap/"}],"title":"BitBase在用户数据分析中的应用与实践","date":"2020/01/19","text":"本文内容来自 数据平台部数据架构组@陈杨 在 BigData NoSQL 12th MeetUp 中的演讲内容。 在快手 HBase 建设的近两年时间中，积累了比较丰富的应用场景：如短视频的存储、IM、直播里评论 feed 流等场景。本次介绍其中的一个场景：Bitbase在用户数据分析中的应用与实践。内容包括业务需求、解决方案、业务效果、未来规划和内部使用经验。 概要 业务需求及挑战：BitBase 引擎的初衷是什么； BitBase 解决方案：在 HBase 基础上，BitBase 的架构； 业务效果：在快手的实际应用场景中，效果如何； 未来规划：中短期的规划； 内部使用经验：快手内部的实际使用经验。 业务需求与挑战1. 业务需求 对于一种技术的应用，首先需要明确其解决的业务需求。本文阐述的技术相应的需求场景用一句话总结：在千亿级日志中，选择任意维度，秒级计算出设备7-90日留存。 如上图所示。左边是原始数据，可能跨90天，每一天的数据可以看作是一张 Hive 宽表，在逻辑上可以认为每行数据的 rowkey 是 userId，需要通过90天的原始数据计算得到右边的表，它的横轴和纵轴都是日期，每个格子表示纵轴日期相对于横轴日期的留存率。 该需求的挑战在于： 日志量大，千亿级； 任意维度，如 city、sex、喜好等，需要选择任意多个维度，在这些维度下计算留存率； 秒级计算，产品面向分析师，等待时间不能过长，最好在1-2秒。 2. 技术选型 调研的技术解决方案如下： Hive： 大部分数仓是基于hive建设的，选择hive的优势在于不需要做大规模数据的迁移和转换，缺点是计算时延过长，会达到小时级别。 ES：通过对原始数据进行倒排索引，然后做一个类似于计算UV的解决方案，但是在数据需要做精确去重的场景下，时延需要秒到分钟级别。 ClickHouse：ClickHouse 是一个比较合适的引擎，也是一个非常优秀的引擎，在业界被广泛应用于 APP 分析，比如漏斗，留存。但是在我们的测试的中，当机器数量比较少时 ( bitmap的格式数据。 2. BitBase架构 BitBase架构主要由五部分组成： 数据存储：存储数据主要是两种，一是bitmap索引及数据，二是转换字典的归档文件，主要用于deviceId和bitmap offsetId的转换。 数据转换：转换有两种方式：一是通过mr任务加工，一是在线计算或导入数据。 数据计算：负责计算任务的解析和调度，以及计算结果返回给client client：将计算逻辑封装成的业务接口，供调用方使用 ZK：负责整个架构的分布式管理 3. 存储模块 BitBase数据历经原始日志数据->Bitmap数据->Hbase表三个阶段的转换操作，最终存储在Hbase表中。Bitmap数据包括两部分： MetaData信息：唯一定位一个 bitmap，db 可以认为是 hive 中的 db，table 也可以认为是 hive 中的 table，event 表示维度 (如:城市)，eventv 表示维度值 (如:bj)，entity 表示 userId（也可能是 photoId），version 表示版本。 BitmapDataHbase表分为三种： BitmapMeta：保存 bitmap 的 meta 信息和一些 block 索引信息。 BlockData：直接保存BitmapData信息 BlockMeta：保存 block 的 meta 信息，起辅助作用。这里注意，相同db的bitmap数据存储在同一张Hbase表中，从而在bitmap计算的时候，可以通过db判断数据的亲和性，同一db的数据计算效率会比较高。 4. 计算模块 一个完整的计算流程包括client、BitBase Server和HBase RegionServer三部分。 BitBase Client将业务需求封装成计算表达式，然后将表达式发送给BitBase Server。从客户端使用的角度来看，bitbase的预留符号有&、|、^、!、@、$、#、%、*，用预留符号表示与、或、非、Count、List、Create等操作。 BitBase Server接收到表达式之后，首先访问BitmapMeta表，查询Block索引，然后将表达式切分成n个子表达式。 如果所有 bitmap 的 db 相同，则走 coprocessor 路由，否则按照数据亲和性，将 block 计算分发到其它 bitbaseServer 中。 根据第3步的调度策略，分两条不同的路径计算 block 表达式。 BitBase Server 聚合 block 计算表达式的结果，然后返回给 BitBase Client。两种计算方式的对比： 非本地计算，解决跨 db 计算的需求，它主要的瓶颈在于网卡和 GC。 本地计算，解决同 db 计算的需求，它主要的瓶颈在 CPU 和 GC 上。整体上看本地计算的性能比非本地计算的性能提高3-5倍，所以要尽量采用本地计算方式。 5. DeviceId问题 一句话描述问题：怎么高效实现字符串类型的DeviceId与long类型的offsetId之间的转换与反转换，并保证offsetId的一致性、连续性。 连续：deviceIdIndex 如果存在空洞，会降低压缩效率，同时 Block 数量会增加，计算复杂度相应增加，最终计算变慢； 一致：deviceId 和 deviceIdIndex 必须是一一对应的，否则计算结果不准确； 反解：根据 deviceIdIndex 能够准确、快速地反解成原始的 deviceId； 转换快：在亿级数据规模下，deviceId 转化为 deviceIdIndex 的过程不能太长。 6. DeviceId方案连续、一致、支持反解： 如何保证连续、一致、支持反解？解决方案非常简单，利用 HBase 实现两阶段提交协议。如上图中间实线部分所示，定义 deviceId 到 deviceIdIndex 的映射为字典。第一张表存储字典的 meta 信息；第二张表存储 index 到 deviceId 的映射；第三张表存储 deviceId 到 index 的映射。 生成 Index 的过程。举例说明, 假设我们已经生成了 1w 个 deviceId 映射，那么此时 f:max=1w，现在将新生成 1k 条映射： 将 f:nextMax=f:max+1k=1.1w； 写 Index 到 deviceId 的反向映射表，1k 条； 写 deviceId 到 Index 的正向映射表，1k 条； 把 f:max=f:nextMax=1.1w 更新到 meta 表，生成过程结束。 如果在生成过程中出现异常或服务器宕机，则执行回滚流程： 如果我们检测到 f:nextMax 不等于 f:max(f:nextMax>f:max)，则从表2中查询 max 到 nextMax 的数据，从表3中删掉相应的 deviceId 到 index 的映射记录； 再删掉表2中相应的 index 到 deviceId 的记录； 最后把 f:nextMax=f:max，从而实现数据100%一致。用 HBase 实现两阶段提交协议要求 index 生成流程和回滚流程一定是单线程的，从而出现性能瓶颈，所以 BitBase 设计了归档流程，以支持快速转换(见后面的描述)。Meta 表中有两个字段，如果发现新产生的数据大于 f:archive_num 就发起归档，把表3中的新数据直接写到 HDFS 中 archive_path 目录下。 采用上述方案与TD的spark程序给offset发号的方案相比，因为增加了监控和回滚方案，所以可靠性更高。 快速转化： 用mr join操作实现转换，转换分deviceId->offsetId和offsetId->deviceId两种，首先看deviceId->offsetId： 同时输入原始数据和字典归档数据，在 MRjob 中根据 deviceId 做 join；判断 deviceId 是否 join 成功； 如果成功了，直接写 hdfs，这样就得到了转化后的数据； 如果 join 失败，直接请求单实例 BitBase Master，BitBase Master 通过两阶段提交协议生成新的映射； 然后返回给 join task 执行替换 deviceId； 把转换后的数据写入 hdfs。 offsetId->deviceId反解的过程很简单，直接多并发读取 HBase。 业务效果1. 时间延迟 如上图所示，第一个图是，两维度、不同时间跨度计算留存的时间延迟；第二个图是15日留存在不同维度上的时延，时延并不会随着维度的增长而增长，原因是维度越多，表达式中可能不需要计算的 block 块也越多。 2. 服务现状 如上图所示，BitBase 可以应用在 app 分析，用户增长，广告 DMP，用户画像等多个业务场景中。 未来规划 根据现在面临的业务场景，BitBase 后续会在多个方面做优化。 支持实时聚合，在一些业务场景下，如运营效果监测，导入时效需要 构建bitmap”的重复操作，加重gc问题。下一步打算直接缓存bitmap。 数据过期与删除问题。在使用实践中，我们发现用户数据的过期方式有两种：第一种是按照bitmap的版本个数进行删除，比如: 只保留bitmap的最先1个版本；第二种是按照导入数据的时间，用TTL来判断过期。由于一个bitmap不能完全映射为一个HBase cell，所以这两种过期方式不能完全映射为HBase的多version控制模式和TTL过期模式。我们现在支持手动执行bitmap的version过期和TTL过期清理，维护成本比较高，后续会支持用户自助配置，系统自动清理的方式。 个人疑问 GreenPlum也能提供大数据量Bitmap的交叉运算及查询交互，不知道有没有对GP这种解决方案做过调研，或者是处于什么原因没有直接使用GP求解。个人思考：公司内部对GP并不是很了解，Hbase整体应用比较成熟，而且基于Hbase可以更加灵活的定制字典转换表等方案，更有利于后期功能和效率的改进。","permalink":"http://renzehello.github.io/2020/01/19/bitbase-in-kwai/","photos":[]},{"tags":[{"name":"Bitmap","slug":"Bitmap","permalink":"http://renzehello.github.io/tags/Bitmap/"}],"title":"Bitmap详解","date":"2020/01/18","text":"导读本文将以 Bitmap 在 TD 内部的使用场景为切入点，较为深入地探讨不同类型 BitMap 算法的存储结构和基本操作，同时会对不同 Bitmap 的优缺点进行简单介绍。希望通过对 Bitmap 的学习，帮助大家在使用bitmap 进行海量数据处理的时候，能够知其然更知其所以然。 参考文献[1] D.Lemire, O. Kaser, K. Aouiche, Sorting improves word-aligned bitmap indexes, Sorting improves word-aligned bitmap indexes.[2] S. Chambi, D. Lemire, O. Kaser, R. Godin, Better bitmap performance with Roaring bitmaps , Natural Sciences and Engineering Research Council of Canada.[3] D. Lemire, G. Ssi-Yan-Kai, O. Kaser, Consistently faster and smaller compressed bitmaps with Roaring, Natural Sciences and Engineering Research Council of Canada.[4] Jianguo Wang, Chunbin Lin, Yannis Papakonstantinou, Steven Swanson, An Experimental Study of Bitmap Compression vs. Inverted List Compression, SIGMOD ’17.[5] D. Lemire, O. Kaser, K. Aouiche, Sorting improves word-aligned bitmap indexes, Data & Knowledge Engineering.","permalink":"http://renzehello.github.io/2020/01/18/bitmap/","photos":[]},{"tags":[],"title":"image","date":"2020/01/18","text":"插入图片test1 test2 test3 test4官方办法 test5 test6 test7","permalink":"http://renzehello.github.io/2020/01/18/image/","photos":[]},{"tags":[{"name":"Flink","slug":"Flink","permalink":"http://renzehello.github.io/tags/Flink/"}],"title":"Flink编程模型","date":"2020/01/17","text":"抽象级别Flink提供了不同的抽象级别来开发流/批处理程序。 最低级别抽象只提供有状态流（stateful streaming）。它将Process Function嵌入到DataStream API中。它允许用户自由处理来自一个或多个流的事件，并使用一致的容错状态。此外，用户可以注册事件时间和处理时间回调，允许程序实现复杂的计算。 实际上，一般应用并不需要stateful streaming这么底层的抽象，而是采用Core Apis即可，例如DataStrean API（有界/无界的流）和DataSet API（有界的数据集）。这些常用的APIs提供了常见的数据处理构建块（building blocks for data processing），比如用户指定的各种形式的转换、连接、聚合、窗口、状态等。在这些api中处理的数据类型用各自的编程语言表示为类。 Table API 是一个以数据表为中心的声明性DSL，表可以(在表示流时)动态地更改表。Table API遵循(扩展的)关系模型:表有一个附加的schema(类似于关系数据库中的表)，而API提供了类似的操作，如select、project、join、group-by、aggregate等。虽然可以通过UDF进行拓展，但是Table Apis的表现力仍然要比Core Apis要弱，不过它使用起来更简洁。此外Table Api在执行之前会经过规则优化器的优化。Table Apis可以在tables和DataStream/DataSet之间自由转换，允许Table API和DataStream/DataSet API混合使用。这些都很像spark的接口类型。 Flink提供的最高级抽象是SQL。这种抽象在语义和表达性上都类似于Table API，但将程序表示为SQL查询表达式。SQL抽象与Table API紧密交互，SQL查询可以在Table API中定义的表上执行。 这里的接口抽象和spark很像，过后可以在没种抽象下加一段示例代码 程序和数据流 结果上图来看，Flink程序可以视为一个有向无环图，图的起点是data source，节点是各种transfermation operator，边就是streaming，终点是data sink。流（streams）和转换（transformations）是构成Flink程序的基本构建块（The basic building blocks，基本组成）。需要注意的是，Flink的DataSet API也是基于流实现的——稍后会详细介绍。从概念上讲，流是一种（可能永远不会结束）流动的数据记录，而转换是一种操作，它接收一个或者多个流作为输入，并且生产出一个或多个输出流作为结果。当执行时，Flink程序被影射到流式数据流（streaming dataflows），由流和转换操作组成。每个数据流开始于一个或多个数据源（sources），并结束于一个或多个接收器（sinks）。数据流类似于任意有向无环图（directed acyclic graphs DAGs）。 并行数据流 从并行和分布式的视角看Flink程序的话，流–>流分区；转换操作–>操作子任务抽象视角 | 并行视角 | 分类|-|-|-stream | stream partitions | - |transfermation operator | operator subtasks | one to one / redistribution | Flink程序是并行和分布式的。在执行期间，一个流有一个或多个流分区（stream partitions），每个操作符有一个或多个操作子任务（operator subtasks）。operator subtasks彼此独立，并在不同的线程中执行，甚至在不同的机器或容器上执行。operator subtasks的数量为该特定操作（operator）的并行度（parallelism）。 流的并行度总是取决于它的生产操作。同一程序的不同操作可能具有不同级别的并行度。流可以在两个操作符之间以一对一 one to one（或转发 forwarding）模式传输数据，也可以采用重分布* Redistributing* 模式： one to one，如上图的source和map操作，保持数据元素的分区和顺序。 redistrubition，重分布。如上图中的 map() 和 keyBy/window，以及 keyBy/window 和 Sink，会改变流的分区。每个操作子任务根据具体的transfermation operator向不同的目标子任务发送数据。例如keyBy()(通过散列键重新分区)、broadcast()或rebalance()(随机重新分区)。在重分发交换中，关于不同键的聚合结果到达接收器的顺序是不确定的。 窗口（Windows） 窗口（Windows）概念是用于在流计算过程中划分聚合计算对应的事件范围，例如“count over the last 5 minutes”或者“sum of the last 100 elements”。窗口可以是时间驱动的（例如:每30秒），也可以是数据驱动的（例如:每100个元素）。窗口可以分为以下三类：翻滚窗口 tumbling windows（没有重叠）、滑动窗口 sliding windows（有重叠）和会话窗口 session windows（中间有一个不活动的间隙）。 时间（Time） 在流处理程序中提到时间（例如定义 windows）时，可以指不同的时间概念： 事件时间（Event Time） 是创建事件的时间。它通常由事件中的时间戳描述，例如由生产传感器或生产服务附加的时间戳。Flink通过时间戳分配程序timestamp assigners访问事件时间戳。 摄入时间 （Ingestion time） 是事件在源操作符处进入Flink数据流的时间。 处理时间（Processing Time） 是指每个基于时间的操作符执行的本地时间。 有状态操作（Stateful Operations）Flink数据流（dataflow）中有些操作只需要每次独立的处理一个事件，例如Event解析操作；有些操作需要记录多个事件的信息，例如窗口操作，后者即被称为有状态的操作（Stateful Operations）。 在Flink架构体系中，有状态计算可以说是Flink非常重要的特性之一。有状态计算是指在程序计算过程中，在Flink程序内部存储计算产生的中间结果，并提供给后续Function或算子计算结果使用。状态数据可以维系在本地存储中，这里的存储可以是Flink的堆内存或者堆外内存，也可以借助第三方的存储介质，例如Flink中已经实现的RocksDB，当然用户也可以自己实现相应的缓存系统去存储状态信息，以完成更加复杂的计算逻辑。和状态计算不同的是，无状态计算不会存储计算过程中产生的结果，也不会将结果用于下一步计算过程中，程序只会在当前的计算流程中实行计算，计算完成就输出结果，然后下一条数据接入，然后再处理。无状态计算实现的复杂度相对较低，实现起来比较容易，但是无法完成提到的比较复杂的业务场景，例如下面的例子： 我们需要输出每个传感器的每个采集值（数值），但是传感器的数据波动很大，如何对数据处理，使其波动没有那么大。这就需要当前的采集值需要联系之前的采集值，换而言之，就需要知道当前传感器的状态。 用户想按照分钟，小时、天进行聚合计算，求取当前的最大值、均值等聚会指标，这就需要利用状态来维护当前计算过程中产生的结果，例如事件的总数，总和以及最大，最小值。 具体内容后面会开一篇文章细讲 容错-CheckpointsFlink使用流重放和检查点的组合实现容错。检查点与每个输入流中的特定点以及每个操作符的对应状态相关。通过恢复运算符的状态并从检查点重放事件，可以从检查点恢复流数据流，同时保持一致性（恰好一次处理语义）。 检查点间隔是在执行期间用恢复时间（需要重放的事件的数量）来折衷容错开销的手段。 批量流（Batch Streaming）Flink执行批处理程序作为流程序的特殊情况，其中流是有界的（有限数量的元素）。 DataSet在内部被视为数据流。因此，上述概念以相同的方式应用于批处理程序，并且它们适用于流程序，除了少数例外： 批处理程序的容错不使用检查点。通过完全重放流来进行恢复。这是可能的，因为输入有限。这会使成本更多地用于恢复，但使常规处理更便宜，因为它避免了检查点。 DataSet API中的有状态操作使用简化的内存/核外数据结构，而不是键/值索引。 DataSet API引入了特殊的同步（超级步骤）迭代，这些迭代只能在有界流上进行。","permalink":"http://renzehello.github.io/2020/01/17/flink-programming-model/","photos":[]},{"tags":[{"name":"源代码","slug":"源代码","permalink":"http://renzehello.github.io/tags/%E6%BA%90%E4%BB%A3%E7%A0%81/"},{"name":"Flink","slug":"Flink","permalink":"http://renzehello.github.io/tags/Flink/"}],"title":"Flink源码分析（一）：源码环境搭建及模块介绍","date":"2020/01/16","text":"更新至Flink release-1.9.0版本 环境 Java 1.8.0_191 Apache Maven 3.6.1 IntelliJ IDEA 2019.2.1 下载访问Apache Flink Github fork 代码到自己github 克隆代码 git clone https://github.com/apache/flink.git 切换至 release-1.9 分支： git checkout release-1.9 编译 修改maven setting文件 nexus-aliyun *,!jeecg,!jeecg-snapshots,!mapr-releases Nexus aliyun http://maven.aliyun.com/nexus/content/groups/public 编译mvn clean package -DskipTests 项目模块Flink release-1.9 分支，Flink 主要模块： flink-annotations 注解； flink-clients 客户端； flink-connectors Flink 连接器，包括 Kafka、ElasticSearch、Cassandra、HBase、HDFS、Hive、JDBC 等； flink-container 提供对 Docker 和 Flink on Kubernetes 支持； flink-contrib 新模块准备或孵化区域； flink-core Flink 核心代码； flink-dist 提供对分发包支持； flink-filesystems 提供对文件系统的支持，包括 HDFS、S3 等； flink-formats 提供对文件格式的支持，包括 Avro、Parquet、JSON、CSV 等； flink-java Flink 底层 API； flink-libraries 提供对事件处理（Flink CEP）、图处理（Flink Gelly）和状态处理的支持； flink-mesos 提供对 Flink on Mesos 支持； flink-metrics 提供对监控的支持，包括 Graphite、InfluxDB、Prometheus、JMX、SLF4j 等； flink-ml-parent 提供对机器学习的支持； flink-optimizer Flink 优化器； flink-python 提供对 Python 的支持； flink-queryable-state 提供对 Queryable State 支持； flink-quickstart 提供对 Java 和 Scala 工程模板的支持； flink-runtime Flink 运行时； flink-runtime-web Dashboard UI； flink-scala 提供对 Scala 的支持； flink-scala-shell 提供对 Scala Shell 的支持； flink-shaded-curator 提供 Apache Curator 依赖的 shaded 包； flink-state-backends 提供对 RocksDB 状态后端的支持； flink-streaming-java DataStream API； flink-streaming-scala DataStream API 的 Scala 版； flink-table Table API 和 SQL； flink-yarn 提供对 Flink on YARN 支持。 以上模块中，粗体为重点模块，将在之后 Flink 源码分析的博文中详细介绍。 参考 Flink 源码分析（一）：源码环境搭建","permalink":"http://renzehello.github.io/2020/01/16/compile-code/","photos":[]},{"tags":[{"name":"源代码","slug":"源代码","permalink":"http://renzehello.github.io/tags/%E6%BA%90%E4%BB%A3%E7%A0%81/"}],"title":"如何高效的阅读源代码","date":"2020/01/16","text":"如何高效的阅读源代码随着数据工程的发展，应用于不同场景的大数据组件种类层出不穷。对于一种大数据工具或者组件的学习，能够在生产环境下使用，自然是最好的一种学习方式。除此之外，在熟练使用的基础上，对源代码进行阅读，不仅有助于加深当前一种工具的理解，更能帮助大家增加分布式、数据密集系统的知识储备，能够起到触类旁通的效果。 个人谈谈阅读源代码的经验。 第一阶段学习基本使用和基本原理，从应用角度对工具进行了解和学习 这是第一个阶段，首先从学习使用工具开始，从应用层面，对其进行一定了解，比如你可以使用shell-cli或者api进行操作。接下来可以尝试了解它的内部原理，注意，不需要通过阅读源代码了解内部原理，只需看一些博客、书籍，不仅需要知道它的基本架构以及各个模块的功能，而且应该知道其具体的工作流程及工作原理，可以自己在纸上完整画完架构及工作流程，越详细越好。 在这个阶段，建议你多看一些知名博客。如果你有实际项目驱动，那是再好不过了，理论联系实际是最好的学习方法；如果你没有项目驱动，那建议你不要自己一个人闷头学，多跟别人交流，多主动给别人讲讲，最好的学习方式还是“讲给别人听”。 第二阶段从无到入门，开始阅读hadoop源代码 这个阶段是最困苦和漫长的，尤其对于那些没有任何分布式经验的人。 很多人这个阶段没有走完，就放弃了，最后停留在应用层面。 当你把源代码导入eclipse或intellij idea，沏上一杯茶，开始准备优哉游哉地看源代码时，你懵逼了：你展开那数不尽的package和class，觉得无从下手，好不容易找到了入口点，然后你屁颠屁颠地通过eclipse的查找引用功能，顺着类的调用关系一层层找下去，最后迷失在了代码的海洋中，如同你在不尽的压栈，最后栈溢出了，你忘记在最初的位置。很多人经历过上面的过程，最后没有顺利逃出来，而放弃。 如果你正在经历这个过程，我的经验如下： 你需要选择一个稳定的release版本 (可以是最新版本，也可以是任一稳定版本)，在本地将源代码编译打包成功，因为在阅读源代码的过程中，一个随时可以debug的环境，可以使你理解代码的过程更加高效。 其次，需要了解工程的代码模块，知道每一个模块对应的作用，并在阅读源代码过程中，时刻谨记你当前阅读的代码属于哪一个模块，会在哪个组件中执行。之后你需要摸清各个组件的交互协议，也就是分布式中的RPC，你需要对RPC的使用方式有所了解，然后看各模块间的RPC protocol，到此，你把握了系统的骨架，这是接下来阅读源代码的基础； 接着，你要选择一个模块开始阅读，一般选择Client，这个模块相对简单些，会给自己增加信心，为了在阅读代码过程中，不至于迷失自己，建议在纸上画出类的调用关系，边看边画，加深理解。 在这个阶段，建议大家多看一些源代码分析博客和书籍，比如《xxxx术内幕》系列丛书。借助这些博客和书籍，你可以在前人的帮助下，更快地学习源代码，节省大量时间，注意，目前博客和书籍很多，建议大家广泛收集资料，找出最适合自己的参考资料。 这个阶段最终达到的目的，是对源代码整体架构和局部的很多细节，有了一定的了解。以hadoop为例，比如你知道MapReduce Scheduler是怎样实现的，MapReduce shuffle过程中，map端做了哪些事情，reduce端做了哪些事情，是如何实现的，等等。这个阶段完成后，当你遇到问题或者困惑点时，可以迅速地在Hadoop源代码中定位相关的类和具体的函数，通过阅读源代码解决问题，这时候，hadoop源代码变成了你解决问题的参考书。 第三个阶段根据需求，修改源代码。 这个阶段，是验证你阅读源代码成效的时候。你根据leader给你的需求，修改相关代码完成功能模块的开发。在修改源代码过程中，你发现之前阅读源代码仍过于粗糙，这时候你再进一步深入阅读相关代码，弥补第二个阶段中薄弱的部分。当然，很多人不需要经历第三个阶段，仅仅第二阶段就够了：一来能够通过阅读代码解决自己长久以来的技术困惑，满足自己的好奇心，二来从根源上解决解决自己遇到的各种问题。 这个阶段，没有太多的参考书籍或者博客，多跟周围的同事交流，通过代码review和测试，证明自己的正确性。 阅读源代码的目的不一定非是工作的需要，你可以把他看成一种修养，通过阅读源代码，加深自己对分布式系统的理解，培养自己踏实做事的心态。 参考 董的博客","permalink":"http://renzehello.github.io/2020/01/16/how-read-code/","photos":[]},{"tags":[],"title":"page","date":"2020/01/13","text":"","permalink":"http://renzehello.github.io/2020/01/13/page/","photos":[]},{"tags":[{"name":"testTag","slug":"testTag","permalink":"http://renzehello.github.io/tags/testTag/"}],"title":"post","date":"2020/01/13","text":"","permalink":"http://renzehello.github.io/2020/01/13/post/","photos":[]}]}